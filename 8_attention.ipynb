{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e09c5228-90b2-4281-8158-1c8e1627e922",
   "metadata": {},
   "source": [
    "## Chapter 8 - Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b127c-66d4-42ed-abd2-10f2afe51bd9",
   "metadata": {},
   "source": [
    "- This chapter uses [TransformerLens](https://github.com/TransformerLensOrg/TransformerLens) and is easiest to run on Linux\n",
    "- Some code here including the  Attention implementation is borrowed from the excellent [ARENA course](https://arena-chapter1-transformer-interp.streamlit.app/) on transformers and mechanistic interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d3d6b6d-02e4-4fc2-866b-ce6eda8c35d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch as t\n",
    "import transformer_lens\n",
    "from dataclasses import dataclass\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c55991c0-35da-42f8-8887-9f53831bf48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to ensure that our deconsutructed GPT-2 attention layers match the original\n",
    "def load_gpt2_test(cls, gpt2_layer, input):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "    print(\"Input shape:\", input.shape)\n",
    "    output = layer(input)\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    try:\n",
    "        reference_output = gpt2_layer(input)\n",
    "    except:\n",
    "        reference_output = gpt2_layer(input, input, input)\n",
    "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
    "    comparison = t.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\\n\")\n",
    "    assert 1 - (comparison.sum() / comparison.numel()) < 1e-5, \"More than 0.01% of the values are incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0189e0e9-508b-4d75-a02b-7ac4c0d6ef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "## GPT-2 small model configuration\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "    \n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bd4ef29-d8e8-4678-a9f5-738afe4c5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(float(\"-inf\"), dtype=t.float32, device=device))\n",
    "\n",
    "    def forward(self, normalized_resid_pre: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        # Calculate query, key and value vectors\n",
    "        q = (einops.einsum(\n",
    "                normalized_resid_pre, self.W_Q, \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\"\n",
    "            ) + self.b_Q)\n",
    "        k = (einops.einsum(\n",
    "                normalized_resid_pre, self.W_K, \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\"\n",
    "            )+ self.b_K)\n",
    "        v = (einops.einsum(\n",
    "                normalized_resid_pre, self.W_V, \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\"\n",
    "            )+ self.b_V)\n",
    "\n",
    "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
    "        attn_scores = einops.einsum(\n",
    "            q, k, \"batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K\"\n",
    "        )\n",
    "        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head**0.5)\n",
    "        attn_pattern = attn_scores_masked.softmax(-1)\n",
    "\n",
    "        # Take weighted sum of value vectors, according to attention probabilities\n",
    "        z = einops.einsum(\n",
    "            v, attn_pattern, \"batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head\"\n",
    "        )\n",
    "\n",
    "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
    "        attn_out = (\n",
    "            einops.einsum(z, self.W_O, \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\")\n",
    "            + self.b_O\n",
    "        )\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        \"\"\"\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        \"\"\"\n",
    "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
    "        all_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
    "        mask = t.triu(all_ones, diagonal=1).bool()\n",
    "        # Apply the mask to attention scores, then return the masked scores\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4be2d0-634a-4d1d-8097-c8fc24120aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9298320-534a-4264-bacb-5cbd94fab892",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text = \"The American flag is red, white, and\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f69b9-f19d-4b0a-88f1-274383108929",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(reference_text).to(device)\n",
    "logits, cache = model.run_with_cache(tokens) #Run through model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5dbad-dbca-4cb4-88c2-4a7f0938c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache #Print sizes for first few tensorse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee162975-2449-43cf-9cfe-a1d7b064d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e27aba9e-9b56-49ae-af34-621fb684f633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 10, 768])\n",
      "Output shape: torch.Size([1, 10, 768])\n",
      "Reference output shape: torch.Size([1, 10, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_gpt2_test(Attention, model.blocks[0].attn, cache[\"normalized\", 0, \"ln1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aba020-8988-477c-b79f-7b8b5398515a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8713bb1-c910-4085-a01e-151d3f923f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests.test_causal_mask(Attention.apply_causal_mask)\n",
    "# rand_float_test(Attention, [2, 4, 768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d078e14-641b-40d6-bcef-e2e4f7ba3db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c98735-913b-478c-92ef-89d4aa68e17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fefbaf1-4919-485a-a984-1368cceb1a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a98063e-c0e5-4976-97b1-706a02f30054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b471740-d61d-4b7d-a5e4-0bba37278245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
